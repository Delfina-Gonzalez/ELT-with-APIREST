{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3bddc8",
   "metadata": {},
   "source": [
    "# ðŸš€ Proyecto ELT con SpaceX API  \n",
    "## Notebook 01 â€“ ExtracciÃ³n y Carga\n",
    "\n",
    "En este notebook se realiza la **extracciÃ³n de datos desde la API pÃºblica de SpaceX** y el **almacenamiento en Delta Lake**.  \n",
    "\n",
    "Se siguen los pasos de la consigna:  \n",
    "1. ExtracciÃ³n de **2 o mÃ¡s endpoints**.  \n",
    "2. Uso de al menos un **endpoint dinÃ¡mico (actualizable)** y otro **estÃ¡tico**.  \n",
    "3. Guardado en **formato Delta Lake**.  \n",
    "4. AplicaciÃ³n de **extracciÃ³n incremental y full** segÃºn corresponda.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6a3062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root detectado en: c:\\Users\\MONSO\\OneDrive\\Escritorio\\Final-DataEngineering\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# --- IMPORTS DEL PROYECTO ---\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextract\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_data\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_to_delta\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_logger\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --- LOGGER ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MONSO\\OneDrive\\Escritorio\\Final-DataEngineering\\src\\load.py:12\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_partition_path\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Iniciar la sesiÃ³n de Spark\u001b[39;00m\n\u001b[32m      8\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProyecto ELT SpaceX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.extensions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mio.delta.sql.DeltaSparkSessionExtension\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalog.spark_catalog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_to_delta\u001b[39m(df: pd.DataFrame, endpoint_name: \u001b[38;5;28mstr\u001b[39m, incremental: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m, mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     15\u001b[39m     path = get_partition_path(endpoint_name, incremental)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MONSO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MONSO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MONSO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MONSO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MONSO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELDA DE CONFIGURACIÃ“N INICIAL\n",
    "# =========================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# --- RUTA DEL PROYECTO ---\n",
    "# Detecta automÃ¡ticamente la raÃ­z del proyecto buscando la carpeta \"src\"\n",
    "def find_project_root(marker=\"src\"):\n",
    "    path = Path().cwd()\n",
    "    for _ in range(5):  # sube hasta 5 niveles si es necesario\n",
    "        if (path / marker).exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    raise FileNotFoundError(f\"No se encontrÃ³ la carpeta '{marker}' en los niveles superiores.\")\n",
    "\n",
    "project_root = find_project_root()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root detectado en: {project_root}\")\n",
    "\n",
    "# --- IMPORTS DEL PROYECTO ---\n",
    "from src.extract import fetch_data\n",
    "from src.load import save_to_delta\n",
    "from src.utils import setup_logger\n",
    "\n",
    "# --- LOGGER ---\n",
    "logger = setup_logger()\n",
    "logger.info(\"Inicio de ejecuciÃ³n del notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785af7dd",
   "metadata": {},
   "source": [
    "## ðŸ”— Endpoints seleccionados\n",
    "\n",
    "- **DinÃ¡micos:**  \n",
    "  - `launches/latest` â†’ Ãšltimo lanzamiento.  \n",
    "  - `launches/upcoming` â†’ PrÃ³ximos lanzamientos.  \n",
    "\n",
    "- **EstÃ¡ticos:**  \n",
    "  - `rockets` â†’ InformaciÃ³n de cohetes.  \n",
    "  - `dragons` â†’ InformaciÃ³n de cÃ¡psulas Dragon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtracciÃ³n de datos\n",
    "df_latest = fetch_data(\"latest_launch\")\n",
    "df_upcoming = fetch_data(\"upcoming_launches\")\n",
    "df_rockets = fetch_data(\"rockets\")\n",
    "df_dragons = fetch_data(\"dragons\")\n",
    "\n",
    "logger.info(f\"Ãšltimo lanzamiento: {len(df_latest)} registros\")\n",
    "logger.info(f\"PrÃ³ximos lanzamientos: {len(df_upcoming)} registros\")\n",
    "logger.info(f\"Cohetes: {len(df_rockets)} registros\")\n",
    "logger.info(f\"Dragons: {len(df_dragons)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582e388",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Guardado en Delta Lake\n",
    "\n",
    "Se guarda cada DataFrame en formato **Delta Lake**:  \n",
    "- Los **endpoints dinÃ¡micos** (`latest_launch`, `upcoming_launches`) se almacenan con **particiones por fecha (extracciÃ³n incremental)**.  \n",
    "- Los **endpoints estÃ¡ticos** (`rockets`, `dragons`) se guardan en una Ãºnica ruta (extracciÃ³n full).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f948bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado en Delta Lake\n",
    "save_to_delta(df_latest, \"latest_launch\", incremental=True)\n",
    "save_to_delta(df_upcoming, \"upcoming_launches\", incremental=True)\n",
    "save_to_delta(df_rockets, \"rockets\", incremental=False)\n",
    "save_to_delta(df_dragons, \"dragons\", incremental=False)\n",
    "\n",
    "logger.info(\"Todos los datasets fueron guardados correctamente en Delta Lake.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
